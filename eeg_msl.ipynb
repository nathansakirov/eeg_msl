{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ERD/ERS Analysis Pipeline\n",
    "\n",
    "This notebook provides a reusable pipeline to process EEG data for a simple ERD/ERS analysis.\n",
    "It:\n",
    "- Loads pre-processed EEG data for a participant.\n",
    "- Creates and merges events from annotations.\n",
    "- Segments the data into epochs (for pre-set conditions).\n",
    "- Computes timeâ€“frequency representations (TFRs) with Morlet wavelets.\n",
    "- Processes the TFR output to compute ERD/ERS values using baseline normalization,\n",
    "  including averaging across regions of interest (ROIs).\n",
    "- Exports and/or plots the final results.\n",
    "\n",
    "You can change key parameters (e.g., file paths, channel selections, epoch timing, etc.)\n",
    "in the **Parameters** section below.\n",
    "\n"
   ],
   "id": "92441572eb0aa969"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "c22dedc920b937f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parameters\n",
    "\n",
    "Set key parameters for the analysis including file paths, channels, epoch settings, ...:"
   ],
   "id": "46f36bb953bc76bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters (user adjustable)\n",
    "params = {\n",
    "    'participant_files': {\n",
    "        # Change these paths to point to your participant .fif files.\n",
    "        'P1': r'C:\\Users\\...',\n",
    "        'P2': r'C:\\Users\\...',\n",
    "        # Add more participant file paths as needed\n",
    "    },\n",
    "    'event_ids': {\n",
    "        # Map annotation labels to numeric codes, change the following to your event IDs:\n",
    "        'Stimulus/S  3': 3,\n",
    "        'Stimulus/S  4': 4,\n",
    "        'Stimulus/S 11': 11,\n",
    "        'Stimulus/S 12': 12,\n",
    "        'Stimulus/S 13': 13,\n",
    "        'Stimulus/S 30': 30\n",
    "        # Add more stimuli as needed\n",
    "    },\n",
    "    'merge_events': {\n",
    "        # Define groups to merge: for instance, merge 3 and 4 into a single event code\n",
    "        # Choose experimental conditions: this example deals with \"familiar\" versus \"unfamiliar\" motor sequences\n",
    "        'familiar': {'include': [3, 4, 30], 'exclude': [11, 12], 'merge': [3, 4], 'new_code': 1},\n",
    "        'unfamiliar': {'include': [11, 12, 30], 'exclude': [3, 4], 'merge': [11, 12], 'new_code': 2},\n",
    "    },\n",
    "    'epoch_params': {\n",
    "        'tmin': -6.5,\n",
    "        'tmax': 0,\n",
    "        'baseline': (-6.5, -5.5),\n",
    "        'reject_criteria': dict(eeg=150e-6),\n",
    "        'flat_criteria': dict(eeg=5e-6),\n",
    "        'channels': ['F2', 'F4', 'F8', 'F1', 'F3', 'F7', 'Fz', 'FCz']\n",
    "    },\n",
    "    'tfr_params': {\n",
    "        'freqs': np.arange(4, 14, 1),\n",
    "        'n_cycles': 3,\n",
    "        'use_fft': True,\n",
    "        'n_jobs': 1,\n",
    "    },\n",
    "    'roi_definitions': {\n",
    "        # Define ROIs as groups of channels (for averaging later)\n",
    "        'LPF': ['F7', 'F3', 'F1'],\n",
    "        'RPF': ['F2', 'F4', 'F8'],\n",
    "        'CC' : ['Fz', 'FCz']\n",
    "        # Add more ROIs as needed\n",
    "    },\n",
    "    'time_selection': {\n",
    "        # Time windows for baseline vs activation window, adjust as necessary\n",
    "        'baseline_window': (-6.5, -5.5),\n",
    "        'activation_window': (-1.5, 0),\n",
    "        'desired_times': [-6.5, -6.4, -6.3, -6.2, -6.1, -6.0, -5.9, -5.8, -5.7, -5.6, -5.5,\n",
    "                          -1.5, -1.4, -1.3, -1.2, -1.1, -1.0, -0.9, -0.8, -0.7, -0.6, -0.5,\n",
    "                          -0.4, -0.3, -0.2, -0.1, 0]\n",
    "    },\n",
    "    'output_paths': {\n",
    "        # Change to your output path\n",
    "        'export_dir': r'C:\\Users\\...'\n",
    "    }"
   ],
   "id": "81fc636f1110d52b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function definitions\n",
    "Below we define functions for the different steps of the pipeline. Adjust as necessary:\n",
    "\n",
    "loading, event creation, merging and re-labelling, trial segmentation, epoch creation, TFR computation, ERD/ERS transformation"
   ],
   "id": "1adf3fef13053d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_raw_data(filepath):\n",
    "    \"\"\"Load raw data from a .fif file.\"\"\"\n",
    "    raw = mne.io.read_raw_fif(filepath, preload=True)\n",
    "    return raw\n",
    "\n",
    "def create_events(raw, event_id_map):\n",
    "    \"\"\"\n",
    "    Create events from annotations using a provided event_id mapping.\n",
    "    Returns events and a dictionary mapping.\n",
    "    \"\"\"\n",
    "    events, event_dict = mne.events_from_annotations(raw, event_id=event_id_map)\n",
    "    return events, event_dict\n",
    "\n",
    "def merge_and_relabel_events(events, merge_def):\n",
    "    \"\"\"\n",
    "    Given an events array, pick and merge events based on include/exclude criteria.\n",
    "    This function uses mne.pick_events and mne.merge_events.\n",
    "    \"\"\"\n",
    "    # Pick events that match \"include\" and exclude unwanted events.\n",
    "    picked_events = mne.pick_events(events, include=merge_def['include'], exclude=merge_def['exclude'])\n",
    "    # Merge the events in the merge list to new_code:\n",
    "    merged_events = mne.merge_events(picked_events, merge_def['merge'], merge_def['new_code'], replace_events=True)\n",
    "    return merged_events\n",
    "\n",
    "def segment_trials(events, trial_ranges):\n",
    "    \"\"\"\n",
    "    Segment the events array into trials based on provided ranges.\n",
    "    `trial_ranges` should be a list of tuples: (start_index, end_index, new_label)\n",
    "    \"\"\"\n",
    "    trials = np.copy(events)\n",
    "    for start, end, new_label in trial_ranges:\n",
    "        trials[start:end+1, 2] = new_label\n",
    "    return trials\n",
    "\n",
    "def create_epochs(raw, events, event_code, epoch_params, picks):\n",
    "    \"\"\"\n",
    "    Create epochs from raw data given events and epoch parameters.\n",
    "    \"\"\"\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_code,\n",
    "                        tmin=epoch_params['tmin'], tmax=epoch_params['tmax'],\n",
    "                        baseline=epoch_params['baseline'], picks=picks,\n",
    "                        reject=epoch_params['reject_criteria'], flat=epoch_params['flat_criteria'],\n",
    "                        detrend=1, reject_by_annotation=True, preload=True)\n",
    "    return epochs\n",
    "\n",
    "def compute_tfr(epochs, tfr_params):\n",
    "    \"\"\"\n",
    "    Compute time-frequency representations (Morlet wavelets) for given epochs.\n",
    "    Returns power and inter-trial coherence (itc) if requested.\n",
    "    \"\"\"\n",
    "    power, itc = tfr_morlet(epochs, freqs=tfr_params['freqs'],\n",
    "                            n_cycles=tfr_params['n_cycles'],\n",
    "                            use_fft=tfr_params['use_fft'],\n",
    "                            return_itc=True,\n",
    "                            n_jobs=tfr_params['n_jobs'])\n",
    "    return power, itc\n",
    "\n",
    "def process_power_to_ERDS(power, roi_defs, time_sel, export_label, participant, hand='Right', block='B1', condition='Familiar'):\n",
    "    \"\"\"\n",
    "    Process the TFR power data:\n",
    "      - Convert to a pandas DataFrame.\n",
    "      - Limit time output to baseline and activation windows.\n",
    "      - Average across channels for each ROI.\n",
    "      - Smooth data by calculating a rolling mean.\n",
    "      - Select only desired time values.\n",
    "      - Compute baseline power and then calculate ERDS.\n",
    "      - Reshape the DataFrame into a long format with ROI, time, and ERDS columns.\n",
    "\n",
    "    Returns the processed DataFrame.\n",
    "    \"\"\"\n",
    "    df = power.to_data_frame(time_format=None)\n",
    "    # Add identifiers, change as needed\n",
    "    df['Block'] = block\n",
    "    df['Participant'] = participant\n",
    "    df['Hand'] = hand\n",
    "    df['Condition'] = condition\n",
    "\n",
    "    # Step 1: Limit time output to baseline and activation windows\n",
    "    df = df[(df['time'].between(*time_sel['baseline_window'])) | (df['time'].between(*time_sel['activation_window']))]\n",
    "\n",
    "    # Step 2: Merge channels for each ROI and drop original channels\n",
    "    for roi, channels in roi_defs.items():\n",
    "        df[roi] = df[channels].mean(axis=1).astype(float)\n",
    "        df.drop(columns=channels, inplace=True)\n",
    "\n",
    "    # Step 3: Smooth data (here we use a rolling mean with window=50 and then repeat values)\n",
    "    for roi in roi_defs.keys():\n",
    "        mean_col = roi + '_mean'\n",
    "        df[mean_col] = df[roi].rolling(window=50, min_periods=1).mean()\n",
    "        # Expand repeated values\n",
    "        df[mean_col] = np.repeat(df[mean_col].values[::50], 50)[:len(df)]\n",
    "        df[roi] = df[mean_col]\n",
    "        df.drop(columns=[mean_col], inplace=True)\n",
    "\n",
    "    # Step 4: Remove redundant time values\n",
    "    df = df[df['time'].isin(time_sel['desired_times'])]\n",
    "\n",
    "    # Step 5: Compute baseline power per frequency (group by frequency)\n",
    "    baseline_df = df[(df['time'] >= time_sel['baseline_window'][0]) & (df['time'] <= time_sel['baseline_window'][1])]\n",
    "    baseline_power = baseline_df.groupby('freq')[list(roi_defs.keys())].mean().reset_index()\n",
    "    # Rename baseline columns for merging\n",
    "    baseline_power = baseline_power.rename(columns={roi: 'baseline_' + roi for roi in roi_defs.keys()})\n",
    "\n",
    "    # Step 6: Merge and compute ERDS for each ROI\n",
    "    merged = pd.merge(df, baseline_power, on='freq')\n",
    "    for roi in roi_defs.keys():\n",
    "        merged['ERDS_' + roi] = ((merged[roi] - merged['baseline_' + roi]) / merged['baseline_' + roi]) * 100\n",
    "    # Drop extra columns\n",
    "    merged.drop(columns=list(roi_defs.keys()) + ['baseline_' + roi for roi in roi_defs.keys()], inplace=True)\n",
    "\n",
    "    # Step 7: Reshape DataFrame from wide to long format\n",
    "    long_df = pd.melt(merged, id_vars=['time', 'freq', 'Block', 'Participant', 'Hand', 'Condition'],\n",
    "                      var_name='ROI', value_name='ERDS')\n",
    "    # Remove extra text (if any) from ROI names (e.g., remove 'ERDS_' prefix)\n",
    "    long_df['ROI'] = long_df['ROI'].str.replace('ERDS_', '')\n",
    "\n",
    "    # Optionally, drop baseline times if not needed in final dataset\n",
    "    long_df = long_df[(long_df['time'] < time_sel['baseline_window'][0]) | (long_df['time'] > time_sel['baseline_window'][1])]\n",
    "\n",
    "    # Export processed data to CSV\n",
    "    export_path = os.path.join(params['output_paths']['export_dir'], f\"{export_label}_{participant}.csv\")\n",
    "    long_df.to_csv(export_path, index=False)\n",
    "    print(f\"Exported processed data to {export_path}\")\n",
    "\n",
    "    return long_df"
   ],
   "id": "49a286819c2cb24d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Main pipeline execution\n",
    "\n",
    "Here, we execute the pipeline for a given participant:"
   ],
   "id": "dc5031b5b761e349"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_pipeline_for_participant(participant_label, filepath, params):\n",
    "    \"\"\"\n",
    "    Run the full ERD/ERS pipeline for a single participant.\n",
    "    \"\"\"\n",
    "    print(f\"Processing participant {participant_label}\")\n",
    "\n",
    "    # 1. Load data\n",
    "    raw = load_raw_data(filepath)\n",
    "\n",
    "    # 2. Create events from annotations\n",
    "    events, event_dict = create_events(raw, params['event_ids'])\n",
    "\n",
    "    # 3. Create merged events for familiar and unfamiliar conditions\n",
    "    familiar_events = merge_and_relabel_events(events, params['merge_events']['familiar'])\n",
    "    unfamiliar_events = merge_and_relabel_events(events, params['merge_events']['unfamiliar'])\n",
    "\n",
    "    # 4. (Optional) Create target events and segment trials if desired.\n",
    "    # For simplicity, we show the basic event creation. You can add segmentation using segment_trials()\n",
    "    # if you have pre-defined trial ranges.\n",
    "\n",
    "    # 5. Define channel picks based on the provided channel list\n",
    "    picks = mne.pick_channels(raw.info[\"ch_names\"], params['epoch_params']['channels'])\n",
    "\n",
    "    # 6. Create epochs for familiar and unfamiliar conditions\n",
    "    epochs_fam = create_epochs(raw, familiar_events, event_code=params['merge_events']['familiar']['new_code'],\n",
    "                               epoch_params=params['epoch_params'], picks=picks)\n",
    "    epochs_unfam = create_epochs(raw, unfamiliar_events, event_code=params['merge_events']['unfamiliar']['new_code'],\n",
    "                                 epoch_params=params['epoch_params'], picks=picks)\n",
    "\n",
    "    # 7. Compute TFRs for each condition\n",
    "    power_fam, itc_fam = compute_tfr(epochs_fam, params['tfr_params'])\n",
    "    power_unfam, itc_unfam = compute_tfr(epochs_unfam, params['tfr_params'])\n",
    "\n",
    "    # 8. Process power data into ERDS metrics\n",
    "    fam_df = process_power_to_ERDS(power_fam, params['roi_definitions'],\n",
    "                                   params['time_selection'],\n",
    "                                   export_label=\"fam\", participant=participant_label,\n",
    "                                   hand='Right', block='B1', condition='Familiar')\n",
    "    unfam_df = process_power_to_ERDS(power_unfam, params['roi_definitions'],\n",
    "                                     params['time_selection'],\n",
    "                                     export_label=\"unfam\", participant=participant_label,\n",
    "                                     hand='Right', block='B1', condition='Unfamiliar')\n",
    "\n",
    "    # 9. (Optional) Combine the familiar and unfamiliar datasets for further analysis\n",
    "    final_df = pd.concat([fam_df, unfam_df], ignore_index=True)\n",
    "    export_path_final = os.path.join(params['output_paths']['export_dir'], f\"final_{participant_label}.csv\")\n",
    "    final_df.to_csv(export_path_final, index=False)\n",
    "    print(f\"Exported final combined dataset to {export_path_final}\")\n",
    "\n",
    "    # 10. Plot events (for quick inspection)\n",
    "    fig = mne.viz.plot_events(np.concatenate([familiar_events, unfamiliar_events], axis=0),\n",
    "                              event_id={'familiar': params['merge_events']['familiar']['new_code'],\n",
    "                                        'unfamiliar': params['merge_events']['unfamiliar']['new_code']},\n",
    "                              sfreq=raw.info['sfreq'])\n",
    "    plt.show()\n",
    "\n",
    "    # 11. Return final DataFrame if needed\n",
    "    return final_df"
   ],
   "id": "c05864d489b220ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the pipeline for multiple participants\n",
    "\n",
    "For example, run for the first participant (P1). You can look over all keys in params['participant_files']."
   ],
   "id": "6502f99204972939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: run for participant P1\n",
    "participant_label = \"P1\"\n",
    "filepath = params['participant_files'][participant_label]\n",
    "final_dataset = run_pipeline_for_participant(participant_label, filepath, params)"
   ],
   "id": "69c09e6b42d6a51a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "- Review the exported CSV files\n",
    "- Adjust any parameters as necessary (e.g., channel picks, epoch times, frequency ranges, etc.).\n",
    "- Extend the pipeline (e.g., add trial segmentation or additional ROI definitions) as needed.\n",
    "- Repeat for additional participants by looping over the file paths in `params['participant_files']`.\n",
    "\n",
    "This should allow you to easily update and reuse the processing code for future datasets.\n"

   ],
   "id": "973120d8a3203e23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
